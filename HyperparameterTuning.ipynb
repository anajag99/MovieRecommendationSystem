{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b583c1-bc59-4ad9-98c4-4963377f7fe7",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Neural Collaborative Filtering (NCF)\r\n",
    "\r\n",
    "This notebook demonstrates the process of **hyperparameter tuning** for optimizing a **Neural Collaborative Filtering (NCF)** model used to predict movie ratings. The objective is to enhance the performance of the NCF model by systematically exploring and selecting the best hyperparameters for key components of the architecture, such as **embedding sizes**, **learning rates**, **batch sizes**, and **number of epochs**. The notebook begins with data preprocessing, including handling missing values, calculating **weighted ratings**, and encoding categorical variables like **user_id** and **movie_id**. After splitting the data into **training** and **test** sets, we perform hyperparameter tuning using methods like **Grid Search** or **Randomized Search** to find the optimal configuration that minimizes **Mean Absolute Error (MAE)** and improves model generalization. The tuned NCF model is then trained and evaluated, and its performance is compared against the baseline to demonstrate the impact of hyperparameter tuning on the model's predictive accurac.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "552d29d6-c915-42d2-8edc-5fffe5eccf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3418c2d4-6ad2-443b-9e79-a96d4983746a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['budget', 'genres', 'id', 'keywords', 'original_language',\n",
       "       'original_title', 'overview', 'popularity', 'production_companies',\n",
       "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
       "       'spoken_languages', 'status', 'tagline', 'title', 'vote_average',\n",
       "       'vote_count', 'movie_id', 'cast', 'crew', 'year', 'weighted_rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('MoviesData_Processed.csv')\n",
    "\n",
    "# Show the first few rows to get an overview of the dataset\n",
    "df.head()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1ff17-4572-4520-a3ba-627637e19b3b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Same as we saw in model training file, NCF is trained here before moving to hyper parameter tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb635637-dfb6-4ea9-b8e1-7b073313d89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4198\n",
      "Epoch 2, Loss: 0.2443\n",
      "Epoch 3, Loss: 0.1615\n",
      "Epoch 4, Loss: 0.1199\n",
      "Epoch 5, Loss: 0.0980\n",
      "Epoch 6, Loss: 0.0902\n",
      "Epoch 7, Loss: 0.0907\n",
      "Epoch 8, Loss: 0.0891\n",
      "Epoch 9, Loss: 0.0870\n",
      "Epoch 10, Loss: 0.0843\n",
      "Epoch 11, Loss: 0.0865\n",
      "Epoch 12, Loss: 0.0837\n",
      "Epoch 13, Loss: 0.0833\n",
      "Epoch 14, Loss: 0.0832\n",
      "Epoch 15, Loss: 0.0856\n",
      "Epoch 16, Loss: 0.0854\n",
      "Epoch 17, Loss: 0.0856\n",
      "Early stopping triggered\n",
      "Test Loss: 0.0778, Test MAE: 0.0828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==========================\n",
    "# Preprocessing & Data Setup\n",
    "# ==========================\n",
    "\n",
    "# Drop NaN values in vote counts and averages\n",
    "df = df.dropna(subset=['vote_count', 'vote_average'])\n",
    "\n",
    "# Calculate overall mean vote average (C)\n",
    "C = df['vote_average'].mean()\n",
    "\n",
    "# Define m as the 90th percentile of vote_count\n",
    "m = df['vote_count'].quantile(0.90)\n",
    "\n",
    "# We filter out movies that have a vote_count less than m\n",
    "qualified = df[df['vote_count'] >= m].copy()\n",
    "\n",
    "# Function to compute the weighted rating\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m)) * R + (m/(v+m)) * C\n",
    "\n",
    "# We calculate weighted rating and create a new column\n",
    "qualified['weighted_rating'] = qualified.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Sorting movies by weighted rating\n",
    "qualified = qualified.sort_values('weighted_rating', ascending=False)\n",
    "\n",
    "# Generate synthetic user_id and select relevant columns\n",
    "df['user_id'] = np.random.randint(0, 1000, df.shape[0])  # Assign random user IDs\n",
    "\n",
    "ratings = df[['user_id', 'id', 'weighted_rating']].dropna()\n",
    "ratings.rename(columns={'id': 'movie_id'}, inplace=True)\n",
    "\n",
    "# Normalize weighted_rating\n",
    "ratings['weighted_rating'] = (ratings['weighted_rating'] - ratings['weighted_rating'].min()) / \\\n",
    "                             (ratings['weighted_rating'].max() - ratings['weighted_rating'].min())\n",
    "\n",
    "# Clip negative values (if any)\n",
    "ratings['weighted_rating'] = ratings['weighted_rating'].clip(lower=0)\n",
    "\n",
    "# Encode users and movies\n",
    "total_users = ratings['user_id'].nunique()\n",
    "total_movies = ratings['movie_id'].nunique()\n",
    "\n",
    "user2idx = {user: idx for idx, user in enumerate(ratings['user_id'].unique())}\n",
    "movie2idx = {movie: idx for idx, movie in enumerate(ratings['movie_id'].unique())}\n",
    "\n",
    "ratings.loc[:, 'user_id'] = ratings['user_id'].map(user2idx)\n",
    "ratings.loc[:, 'movie_id'] = ratings['movie_id'].map(movie2idx)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==========================\n",
    "# PyTorch Dataset Class\n",
    "# ==========================\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.users = torch.tensor(data['user_id'].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(data['movie_id'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(data['weighted_rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = MovieDataset(train_data)\n",
    "test_dataset = MovieDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# Neural Collaborative Filtering (NCF) Model\n",
    "# ==========================\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_size=64):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embed_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user, movie):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        interaction = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        output = self.fc_layers(interaction)\n",
    "        return output.squeeze()\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ncf_model = NCF(total_users, total_movies).to(device)\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "optimizer = optim.AdamW(ncf_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ==========================\n",
    "# Model Weight Initialization\n",
    "# ==========================\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "ncf_model.apply(weights_init)\n",
    "\n",
    "# ==========================\n",
    "# Training Loop\n",
    "# ==========================\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience, counter = 3, 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for users, movies, ratings in train_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            for param in model.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# ==========================\n",
    "# Evaluation Function\n",
    "# ==========================\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_absolute_error = 0  # Variable to store the sum of absolute errors\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate absolute errors for MAE\n",
    "            absolute_error = torch.abs(predictions - ratings)\n",
    "            total_absolute_error += absolute_error.sum().item()\n",
    "    \n",
    "    # Calculate MAE and average loss\n",
    "    mae = total_absolute_error / len(test_loader.dataset)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# Run Training and Evaluation\n",
    "# ==========================\n",
    "\n",
    "train(ncf_model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
    "evaluate(ncf_model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bfa338-8b4d-4c3b-a7d1-037fa5c3dc17",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering (NCF) with Hyperparameter Optimization\n",
    "\n",
    "This code implements a **Neural Collaborative Filtering (NCF)** model for predicting movie ratings, where the hyperparameters of the model are optimized using **Optuna**. The first part of the code simulates user-item interactions by generating **random user IDs** and **weighted ratings** for the movies. After selecting and normalizing the ratings, user and movie IDs are **encoded** to create unique mappings. The data is then split into **train** and **test** sets, and a **PyTorch Dataset** class is used for data handling. The core model, `NCF`, is defined with **embedding layers** for both users and movies, followed by several **fully connected layers** with **dropout** for regularization. The **objective function** of the hyperparameter optimization process utilizes Optuna to sample various hyperparameters such as embedding size, layer configuration, dropout rate, learning rate, batch size, and weight decay. The model is trained for **10 epochs**, and the performance is evaluated using **mean absolute error (L1 loss)**. Finally, Optuna searches for the **best hyperparameters** by minimizing the test loss over a set of trials, with the best combination of hyperparameters printed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8749096-da5a-4ff0-b41d-b0524d081355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_26048\\643855626.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings.rename(columns={'id': 'movie_id'}, inplace=True)\n",
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_26048\\643855626.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['weighted_rating'] = (ratings['weighted_rating'] - 1) / 4\n",
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_26048\\643855626.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['user_id'] = ratings['user_id'].map(user2idx)\n",
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_26048\\643855626.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['movie_id'] = ratings['movie_id'].map(movie2idx)\n",
      "[I 2025-03-31 23:39:35,591] A new study created in memory with name: no-name-7f26f4d2-8bde-4782-8eec-fc4bd0de74d1\n",
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_26048\\643855626.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 0.0005, 0.005)\n",
      "C:\\Users\\anagh\\AppData\\Local\\Temp\\ipykernel_26048\\643855626.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
      "[I 2025-03-31 23:39:39,876] Trial 0 finished with value: 0.34389394149184227 and parameters: {'embed_size': 128, 'num_layers': 2, 'layer_0': 224, 'layer_1': 64, 'dropout': 0.1363947276753384, 'lr': 0.0006266069418655011, 'batch_size': 128, 'weight_decay': 1.7881882471062398e-05}. Best is trial 0 with value: 0.34389394149184227.\n",
      "[I 2025-03-31 23:39:50,519] Trial 1 finished with value: 0.34042656133251803 and parameters: {'embed_size': 64, 'num_layers': 2, 'layer_0': 256, 'layer_1': 128, 'dropout': 0.1184493797147427, 'lr': 0.0008608648945518017, 'batch_size': 32, 'weight_decay': 0.00680309414818449}. Best is trial 1 with value: 0.34042656133251803.\n",
      "[I 2025-03-31 23:39:54,293] Trial 2 finished with value: 0.321216506883502 and parameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 32, 'layer_1': 32, 'dropout': 0.14139695765954396, 'lr': 0.001825173038756251, 'batch_size': 64, 'weight_decay': 0.0017154082742364917}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:39:56,449] Trial 3 finished with value: 0.3417474366724491 and parameters: {'embed_size': 64, 'num_layers': 1, 'layer_0': 64, 'dropout': 0.18741511443969494, 'lr': 0.0011285714370028962, 'batch_size': 128, 'weight_decay': 0.000713057424043751}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:07,917] Trial 4 finished with value: 0.35043249976250435 and parameters: {'embed_size': 64, 'num_layers': 3, 'layer_0': 64, 'layer_1': 160, 'layer_2': 64, 'dropout': 0.19965354244538566, 'lr': 0.0012052116884660648, 'batch_size': 32, 'weight_decay': 1.20792706458858e-05}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:11,602] Trial 5 finished with value: 0.3291857987642288 and parameters: {'embed_size': 32, 'num_layers': 3, 'layer_0': 64, 'layer_1': 224, 'layer_2': 32, 'dropout': 0.1311870702494779, 'lr': 0.0008968092984726547, 'batch_size': 128, 'weight_decay': 1.648220871513401e-05}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:16,326] Trial 6 finished with value: 0.350669015198946 and parameters: {'embed_size': 32, 'num_layers': 1, 'layer_0': 64, 'dropout': 0.23434094485486892, 'lr': 0.0024073231176109963, 'batch_size': 64, 'weight_decay': 0.00028383737571444794}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:25,295] Trial 7 finished with value: 0.34936883757191317 and parameters: {'embed_size': 64, 'num_layers': 1, 'layer_0': 64, 'dropout': 0.16314386303076223, 'lr': 0.0029207660285076745, 'batch_size': 32, 'weight_decay': 0.0001618664258302071}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:31,581] Trial 8 finished with value: 0.3439665623009205 and parameters: {'embed_size': 32, 'num_layers': 3, 'layer_0': 128, 'layer_1': 224, 'layer_2': 32, 'dropout': 0.18262095205662132, 'lr': 0.0009360651025368229, 'batch_size': 64, 'weight_decay': 2.427644975775011e-05}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:41,858] Trial 9 finished with value: 0.35424220273571627 and parameters: {'embed_size': 32, 'num_layers': 3, 'layer_0': 192, 'layer_1': 32, 'layer_2': 128, 'dropout': 0.167571496231562, 'lr': 0.0015575364662139162, 'batch_size': 32, 'weight_decay': 9.148252355676828e-05}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:50,136] Trial 10 finished with value: 0.3535798955708742 and parameters: {'embed_size': 128, 'num_layers': 2, 'layer_0': 128, 'layer_1': 96, 'dropout': 0.28432616877336186, 'lr': 0.004642344521842774, 'batch_size': 64, 'weight_decay': 0.0038202712204175033}. Best is trial 2 with value: 0.321216506883502.\n",
      "[I 2025-03-31 23:40:53,163] Trial 11 finished with value: 0.31776605173945427 and parameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 32, 'layer_1': 256, 'dropout': 0.11881618201297707, 'lr': 0.0005645809761012218, 'batch_size': 128, 'weight_decay': 0.001361452752121173}. Best is trial 11 with value: 0.31776605173945427.\n",
      "[I 2025-03-31 23:40:56,147] Trial 12 finished with value: 0.31931566074490547 and parameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 32, 'layer_1': 256, 'dropout': 0.1002235633856868, 'lr': 0.000556000406042709, 'batch_size': 128, 'weight_decay': 0.001667183228018126}. Best is trial 11 with value: 0.31776605173945427.\n",
      "[I 2025-03-31 23:40:59,116] Trial 13 finished with value: 0.3171287402510643 and parameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 32, 'layer_1': 256, 'dropout': 0.10249461756578146, 'lr': 0.0005091698633743187, 'batch_size': 128, 'weight_decay': 0.0010345902844456764}. Best is trial 13 with value: 0.3171287402510643.\n",
      "[I 2025-03-31 23:41:03,175] Trial 14 finished with value: 0.31656423956155777 and parameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 160, 'layer_1': 256, 'dropout': 0.2351776451471082, 'lr': 0.0005784837063420347, 'batch_size': 128, 'weight_decay': 0.0006577726351663704}. Best is trial 14 with value: 0.31656423956155777.\n",
      "[I 2025-03-31 23:41:06,452] Trial 15 finished with value: 0.32291193678975105 and parameters: {'embed_size': 32, 'num_layers': 1, 'layer_0': 160, 'dropout': 0.23526730306758245, 'lr': 0.0007466805479227988, 'batch_size': 128, 'weight_decay': 0.0004482852986954407}. Best is trial 14 with value: 0.31656423956155777.\n",
      "[I 2025-03-31 23:41:12,111] Trial 16 finished with value: 0.33701516315340996 and parameters: {'embed_size': 128, 'num_layers': 2, 'layer_0': 160, 'layer_1': 192, 'dropout': 0.23155660872083764, 'lr': 0.0005035263855806284, 'batch_size': 128, 'weight_decay': 8.460025553614126e-05}. Best is trial 14 with value: 0.31656423956155777.\n",
      "[I 2025-03-31 23:41:14,800] Trial 17 finished with value: 0.32599565759301186 and parameters: {'embed_size': 32, 'num_layers': 1, 'layer_0': 96, 'dropout': 0.2753256744543577, 'lr': 0.0006704912947103119, 'batch_size': 128, 'weight_decay': 0.0007135371288619695}. Best is trial 14 with value: 0.31656423956155777.\n",
      "[I 2025-03-31 23:41:19,437] Trial 18 finished with value: 0.33462825044989586 and parameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 192, 'layer_1': 192, 'dropout': 0.2644568450490023, 'lr': 0.001257466332489426, 'batch_size': 128, 'weight_decay': 0.0034626646302566883}. Best is trial 14 with value: 0.31656423956155777.\n",
      "[I 2025-03-31 23:41:24,266] Trial 19 finished with value: 0.3487030640244484 and parameters: {'embed_size': 128, 'num_layers': 3, 'layer_0': 192, 'layer_1': 256, 'layer_2': 256, 'dropout': 0.217579089316606, 'lr': 0.0007360473600966483, 'batch_size': 128, 'weight_decay': 0.00022746943886160285}. Best is trial 14 with value: 0.31656423956155777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'embed_size': 32, 'num_layers': 2, 'layer_0': 160, 'layer_1': 256, 'dropout': 0.2351776451471082, 'lr': 0.0005784837063420347, 'batch_size': 128, 'weight_decay': 0.0006577726351663704}\n"
     ]
    }
   ],
   "source": [
    "# Simulate user-item interactions (Assuming a rating-like system based on popularity)\n",
    "df['user_id'] = np.random.randint(0, 1000, df.shape[0])  \n",
    "df['weighted_rating'] = np.random.randint(1, 6, df.shape[0])  # This is your custom rating system\n",
    "\n",
    "# Select relevant columns\n",
    "ratings = df[['user_id', 'id', 'weighted_rating']]\n",
    "ratings.rename(columns={'id': 'movie_id'}, inplace=True)\n",
    "\n",
    "# Normalize ratings to [0, 1]\n",
    "ratings['weighted_rating'] = (ratings['weighted_rating'] - 1) / 4  \n",
    "\n",
    "# Encode users and movies\n",
    "total_users = ratings['user_id'].nunique()\n",
    "total_movies = ratings['movie_id'].nunique()\n",
    "user2idx = {user: idx for idx, user in enumerate(ratings['user_id'].unique())}\n",
    "movie2idx = {movie: idx for idx, movie in enumerate(ratings['movie_id'].unique())}\n",
    "\n",
    "ratings['user_id'] = ratings['user_id'].map(user2idx)\n",
    "ratings['movie_id'] = ratings['movie_id'].map(movie2idx)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Dataset class\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.users = torch.tensor(data['user_id'].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(data['movie_id'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(data['weighted_rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# Model with Hyperparameter Optimization\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_size, hidden_layers, dropout):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_size)\n",
    "        \n",
    "        layers = []\n",
    "        input_size = embed_size * 2\n",
    "        for units in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = units\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user, movie):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        interaction = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        output = self.fc_layers(interaction)\n",
    "        return output.squeeze()\n",
    "\n",
    "# Hyperparameter Optimization Function\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    embed_size = trial.suggest_categorical(\"embed_size\", [32, 64, 128])\n",
    "    hidden_layers = [trial.suggest_int(f\"layer_{i}\", 32, 256, step=32) for i in range(trial.suggest_int(\"num_layers\", 1, 3))]\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 0.0005, 0.005)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(MovieDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(MovieDataset(test_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = NCF(total_users, total_movies, embed_size, hidden_layers, dropout).to(device)\n",
    "    criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Using 10 epochs per trial for quick evaluation\n",
    "        total_loss = 0\n",
    "        for users, movies, ratings in train_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    return test_loss  # Optuna minimizes this loss\n",
    "\n",
    "# Run Optuna Hyperparameter Tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print Best Hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205aba70-5a5c-4380-8f66-97d7fedb335d",
   "metadata": {},
   "source": [
    "## Model Retraining with suggested hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ef87793-c4e9-4772-9906-3ebde0118306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5500\n",
      "Epoch 2, Loss: 0.3830\n",
      "Epoch 3, Loss: 0.3663\n",
      "Epoch 4, Loss: 0.3543\n",
      "Epoch 5, Loss: 0.3427\n",
      "Epoch 6, Loss: 0.3284\n",
      "Epoch 7, Loss: 0.3176\n",
      "Epoch 8, Loss: 0.3130\n",
      "Epoch 9, Loss: 0.3153\n",
      "Epoch 10, Loss: 0.3135\n",
      "Epoch 11, Loss: 0.3059\n",
      "Epoch 12, Loss: 0.3081\n",
      "Epoch 13, Loss: 0.3039\n",
      "Epoch 14, Loss: 0.3010\n",
      "Epoch 15, Loss: 0.3000\n",
      "Epoch 16, Loss: 0.3010\n",
      "Epoch 17, Loss: 0.2932\n",
      "Epoch 18, Loss: 0.2976\n",
      "Epoch 19, Loss: 0.2996\n",
      "Epoch 20, Loss: 0.3000\n",
      "Early stopping triggered\n",
      "Test Loss: 0.3167, Test MAE: 0.3173\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Preprocessing & Data Setup\n",
    "# ==========================\n",
    "\n",
    "# Assume df is already loaded with movie data\n",
    "# Drop NaN values in vote counts and averages\n",
    "df = df.dropna(subset=['vote_count', 'vote_average'])\n",
    "\n",
    "# Calculate overall mean vote average (C)\n",
    "C = df['vote_average'].mean()\n",
    "\n",
    "# Define m as the 90th percentile of vote_count\n",
    "m = df['vote_count'].quantile(0.90)\n",
    "\n",
    "# We filter out movies that have a vote_count less than m\n",
    "qualified = df[df['vote_count'] >= m].copy()\n",
    "\n",
    "# Function to compute the weighted rating\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m)) * R + (m/(v+m)) * C\n",
    "\n",
    "# We calculate weighted rating and create a new column\n",
    "qualified['weighted_rating'] = qualified.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Sorting movies by weighted rating\n",
    "qualified = qualified.sort_values('weighted_rating', ascending=False)\n",
    "\n",
    "# Generate synthetic user_id and select relevant columns\n",
    "df['user_id'] = np.random.randint(0, 1000, df.shape[0])  # Assign random user IDs\n",
    "\n",
    "ratings = df[['user_id', 'id', 'weighted_rating']].dropna()\n",
    "ratings.rename(columns={'id': 'movie_id'}, inplace=True)\n",
    "\n",
    "# Normalize weighted_rating\n",
    "ratings['weighted_rating'] = (ratings['weighted_rating'] - ratings['weighted_rating'].min()) / \\\n",
    "                             (ratings['weighted_rating'].max() - ratings['weighted_rating'].min())\n",
    "\n",
    "# Clip negative values (if any)\n",
    "ratings['weighted_rating'] = ratings['weighted_rating'].clip(lower=0)\n",
    "\n",
    "# Encode users and movies\n",
    "total_users = ratings['user_id'].nunique()\n",
    "total_movies = ratings['movie_id'].nunique()\n",
    "\n",
    "user2idx = {user: idx for idx, user in enumerate(ratings['user_id'].unique())}\n",
    "movie2idx = {movie: idx for idx, movie in enumerate(ratings['movie_id'].unique())}\n",
    "\n",
    "ratings.loc[:, 'user_id'] = ratings['user_id'].map(user2idx)\n",
    "ratings.loc[:, 'movie_id'] = ratings['movie_id'].map(movie2idx)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==========================\n",
    "# PyTorch Dataset Class\n",
    "# ==========================\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.users = torch.tensor(data['user_id'].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(data['movie_id'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(data['weighted_rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = MovieDataset(train_data)\n",
    "test_dataset = MovieDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# Neural Collaborative Filtering (NCF) Model\n",
    "# ==========================\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_size=32, hidden_layers=[160, 256], dropout=0.2351776451471082):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_size)\n",
    "        \n",
    "        layers = []\n",
    "        input_size = embed_size * 2\n",
    "        for units in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = units\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user, movie):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        interaction = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        output = self.fc_layers(interaction)\n",
    "        return output.squeeze()\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ncf_model = NCF(total_users, total_movies, embed_size=32, hidden_layers=[160, 256], dropout=0.2351776451471082).to(device)\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "optimizer = optim.AdamW(ncf_model.parameters(), lr=0.0005784837063420347, weight_decay=0.0006577726351663704)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ==========================\n",
    "# Model Weight Initialization\n",
    "# ==========================\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "ncf_model.apply(weights_init)\n",
    "\n",
    "# ==========================\n",
    "# Training Loop\n",
    "# ==========================\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience, counter = 3, 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for users, movies, ratings in train_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            for param in model.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# ==========================\n",
    "# Evaluation Function\n",
    "# ==========================\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_absolute_error = 0  # Variable to store the sum of absolute errors\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate absolute errors for MAE\n",
    "            absolute_error = torch.abs(predictions - ratings)\n",
    "            total_absolute_error += absolute_error.sum().item()\n",
    "    \n",
    "    # Calculate MAE and average loss\n",
    "    mae = total_absolute_error / len(test_loader.dataset)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# Run Training and Evaluation\n",
    "# ==========================\n",
    "\n",
    "train(ncf_model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
    "evaluate(ncf_model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a04a8-d833-4123-8833-4329f0b7964b",
   "metadata": {},
   "source": [
    "### Addressing the Scenario: Post-Hyperparameter Tuning Performance Degradation\n",
    "\n",
    "After performing hyperparameter tuning using **Optuna**, the model results showed that despite a **decrease in training loss** over the epochs, the **test loss** and **Mean Absolute Error (MAE)** increased. This indicates potential **overfitting**, where the model is performing well on the training data but struggles to generalize to unseen data. In this case, despite improvements during training, the model's ability to predict accurately on the test set has diminished. \n",
    "**Future Scope**: To address this issue, several potential solutions could be explored, first, **regularization techniques** such as **early stopping** (which was already triggered) could be fine-tuned to avoid overfitting, adjusting the **dropout rate** or exploring **weight decay** more rigorously. Additionally, increasing the **amount of training data**, using a different **model architecture**, or reviewing the **hyperparameters** (such as embedding sizes and learning rates) to ensure they are balanced and not too aggressive could help mitigate this issue. Finally, it would be helpful to conduct **cross-validation** during hyperparameter search to better evaluate the model's generalization capability before finalizing the tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf586f-4557-4e03-88f1-c06734300791",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "173d71d9-073d-4d6b-bcf8-26edd4a6b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(ncf_model.state_dict(), 'ncf_model.pth')\n",
    "\n",
    "# Save the optimizer state\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "\n",
    "print(\"Model and optimizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffab90-5ea3-4809-939e-306d59b351b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
