{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1f9be9-e916-4bbe-81f9-03145951d8e5",
   "metadata": {},
   "source": [
    "# Model Training\r\n",
    "\r\n",
    "In this section, we perform the **model training** for three different recommendation systems:\r\n",
    "\r\n",
    "1. **Collaborative Filtering** using **KNN** and **NCF**\r\n",
    "2. **Content-based Filtering** using **TF-IDF**\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Overview of the Models\r\n",
    "\r\n",
    "- **Content-based Filtering**:  \r\n",
    "  The **TF-IDF** model recommends movies based on the textual content of movie descriptions (such as the **title** and **overview**). It uses these columns to compute similarity scores between movies and recommend similar ones.\r\n",
    "\r\n",
    "- **Collaborative Filtering**:  \r\n",
    "  This method predicts whether a movie should be recommended to a user based on **numerical** columns like `user_id`, `movie_id`, and **ratings**. Since our dataset lacks actual `user_id` and `ratings` columns, we generate **synthetic `user_id` values**. Instead of actual ratings, we use the **`weighted_rating`** column, which is calculated during **feature engineering**, to simulate user preferences.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Steps Involved\r\n",
    "1. **Loading the Dataset**  \r\n",
    "   We load the dataset containing movie data and preprocess it for model training.\r\n",
    "\r\n",
    "2. **Generating Synthetic Data**  \r\n",
    "   Since the dataset does not contain **`user_id`** or **`ratings`**, we create synthetic **`user_id`** values. The **`weighted_rating`** is used in place of the missing ratings to simulate movie preferences.\r\n",
    "\r\n",
    "3. **Training Models**  \r\n",
    "   We train three models:\r\n",
    "   - **KNN** for Collaborative Filtering\r\n",
    "   - **NCF** (Neural Collaborative Filtering) for Collaborative Filtering\r\n",
    "   - **TF-IDF** for Content-based Filtering\r\n",
    "\r\n",
    "Each model is trained on the preprocessed data to learn the relationships and provide recommendations.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "By the end of this notebook, we evaluate the models based on their performance and determine the best recommendation system for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6684404-017f-48cf-901a-c4b69abf6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b3df767-0bff-491f-b468-5781eb3d15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"MoviesData_Processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb2d03-f3a3-4f15-af5b-c6f6670c884e",
   "metadata": {},
   "source": [
    "### Content-Based Movie Recommendation System\r\n",
    "\r\n",
    "In this section, we implement a **Content-Based Filtering** recommendation system using **TF-IDF (Term Frequency-Inverse Document Frequency)** and **Cosine Similarity** to suggest movies based on their **overview**.\r\n",
    "\r\n",
    "1. **Preprocessing**:  \r\n",
    "   The first step involves filling any missing values in the **`overview`** column with an empty string to ensure the **TF-IDF Vectorizer** works smoothly without errors from `NaN` values.\r\n",
    "\r\n",
    "2. **TF-IDF Vectorization**:  \r\n",
    "   A **TF-IDF Vectorizer** is initialized with the `stop_words='english'` parameter to remove common English words (e.g., \"the\", \"is\", etc.) that do not contribute much to the similarity calculation. This vectorizer is then applied to the **`overview`** column to create a **TF-IDF matrix**, where each row corresponds to a movie's overview and each column represents a word weighted by its importance across the corpus.\r\n",
    "\r\n",
    "3. **Cosine Similarity Calculation**:  \r\n",
    "   Next, the **Cosine Similarity** between each movie's overview is computed using the **TF-IDF matrix**. This step measures how similar the overviews of the movies are to one another. A higher cosine similarity indicates a greater similarity between two movies.\r\n",
    "\r\n",
    "4. **Reverse Mapping for Movie Titles**:  \r\n",
    "   A reverse mapping of **movie titles** to their corresponding **index** in the dataset is created using a **pandas Series**. This allows us to quickly look up the index of a movie given its title.\r\n",
    "\r\n",
    "5. **Recommendation Function**:  \r\n",
    "   The `content_based_recommendations` function is designed to take a **movie title** as input, find its index in the dataset, and retrieve a list of **top_n most similar movies** based on the cosine similarity. If the provided title is not found in the dataset, it prints a message and returns an empty list. Otherwise, it sorts the movies by similarity score, excludes the input movie itself, and returns the **top_n** most similar movies along with their overviews.\r\n",
    "\r\n",
    "6. **Example Usage**:  \r\n",
    "   An example usage of the `content_based_recommendations` function is provided with the movie title **\"Spider-Man 2\"**. The function will return a list of the top 10 movies most similar to \"Spider-Man 2\" based on their **overvuses\n",
    "This method leverages the **TF-IDF** technique to identify movies with similar descriptions and recommend them to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e5974bf-57a8-4fee-87f7-99180f4a849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                original_title  \\\n",
      "5                 Spider-Man 3   \n",
      "159                 Spider-Man   \n",
      "20      The Amazing Spider-Man   \n",
      "38    The Amazing Spider-Man 2   \n",
      "1540             Arachnophobia   \n",
      "3080        The House of Mirth   \n",
      "813                   Superman   \n",
      "1566                27 Dresses   \n",
      "2002       Someone Like You...   \n",
      "3792        Psycho Beach Party   \n",
      "\n",
      "                                               overview  \n",
      "5     The seemingly invincible Spider-Man goes up ag...  \n",
      "159   After being bitten by a genetically altered sp...  \n",
      "20    Peter Parker is an outcast high schooler aband...  \n",
      "38    For Peter Parker, life is busy. Between taking...  \n",
      "1540  A large spider from the jungles of South Ameri...  \n",
      "3080  A woman risks losing her chance of happiness w...  \n",
      "813   Mild-mannered Clark Kent works as a reporter a...  \n",
      "1566  Altruistic Jane finds herself facing her worst...  \n",
      "2002  Jane Goodale has everything going for her. She...  \n",
      "3792  Spoof of 1960's Beach Party/Gidget surfing mov...  \n"
     ]
    }
   ],
   "source": [
    "df['overview'] = df['overview'].fillna('')\n",
    "\n",
    "# Initializing TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(df['overview'])\n",
    "\n",
    "# Computing the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Constructing a reverse map of indices and movie titles\n",
    "indices = pd.Series(df.index, index=df['original_title']).drop_duplicates()\n",
    "\n",
    "def content_based_recommendations(title, cosine_sim=cosine_sim, df=df, indices=indices, top_n=10):\n",
    "    idx = indices.get(title)\n",
    "    if idx is None:\n",
    "        print(\"Title not found in the dataset.\")\n",
    "        return []\n",
    "    \n",
    "    # Getting the pairwise similarity scores for this movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    # Sorting the movies based on similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the scores of the top_n most similar movies (excluding itself)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "\n",
    "    return df[['original_title', 'overview']].iloc[movie_indices]\n",
    "\n",
    "# Example usage:\n",
    "print(content_based_recommendations(\"Spider-Man 2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d79cc5-67c1-4b22-8384-2e6a952f87e1",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering (NCF) with Movie Recommendations\n",
    "\n",
    "In this section, we implement a **Neural Collaborative Filtering (NCF)** model to recommend movies based on synthetic user ratings and movie data. The model follows several key steps, including data preprocessing, model setup, training, and evaluation.\n",
    "\n",
    "1. **Preprocessing & Data Setup**:  \n",
    "   The data is first cleaned by dropping rows with missing **`vote_count`** or **`vote_average`** values. An overall mean **C** of the vote average is calculated, and **`m`**, the 90th percentile of the **`vote_count`**, is computed to filter out movies with fewer than **m** votes. We then compute a **weighted rating** for each movie, which considers both the movie's rating and its popularity. Synthetic **`user_id`** values are generated for each movie, and the dataset is normalized and split into **train** and **test** sets.\n",
    "\n",
    "2. **PyTorch Dataset Class**:  \n",
    "   A custom PyTorch `Dataset` class, `MovieDataset`, is defined to handle the conversion of the data into a format suitable for deep learning. This class takes in the user IDs, movie IDs, and their corresponding **weighted ratings**, and formats them into tensors for efficient loading during training and testing.\n",
    "\n",
    "3. **NCF Model Architecture**:  \n",
    "   The **Neural Collaborative Filtering (NCF)** model is implemented using **embedding layers** for both users and movies. The embeddings are concatenated and passed through a series of **fully connected layers** with dropout for regularization, producing a final predicted rating. This architecture is designed to learn complex interactions between users and movies through embeddings.\n",
    "\n",
    "4. **Model Initialization**:  \n",
    "   The model is initialized with the appropriate number of user and movie embeddings, along with the loss function (`L1Loss` for mean absolute error) and the optimizer (`AdamW`). The learning rate scheduler is set to decrease the learning rate every 5 epochs by a factor of 0.5 to help with convergence.\n",
    "\n",
    "5. **Weight Initialization**:  \n",
    "   The weights of the model’s **fully connected layers** are initialized using **Xavier uniform** initialization to improve training stability.\n",
    "\n",
    "6. **Training Loop**:  \n",
    "   The training loop iterates over the data for a specified number of epochs. During each epoch, the model predicts ratings, computes the loss, and performs backpropagation to update the model parameters. Gradient clipping is used to prevent exploding gradients. Early stopping is applied to halt training if the loss does not improve for several epochs.\n",
    "\n",
    "7. **Evaluation**:  \n",
    "   After training, the model is evaluated on the **test set** to calculate the loss and **Mean Absolute Error (MAE)**, which measures how well the model’s predictions align with the actual ratings.\n",
    "\n",
    "8. **Run Training and Evaluation**:  \n",
    "   Finally, the model is trained for 20 epochs and then evaluated on the test data, with both the loss and MAE being printed out.\n",
    "\n",
    "This implementation demonstrates how neural collaborative filtering can be applied for movie recommendation tasks by leveraging deep learning techniques to model the interactions between users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63df3693-0923-4e9e-9a0b-fd128cc081c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7332\n",
      "Epoch 2, Loss: 0.6341\n",
      "Epoch 3, Loss: 0.5369\n",
      "Epoch 4, Loss: 0.4552\n",
      "Epoch 5, Loss: 0.4455\n",
      "Epoch 6, Loss: 0.4136\n",
      "Epoch 7, Loss: 0.3913\n",
      "Epoch 8, Loss: 0.3676\n",
      "Epoch 9, Loss: 0.3320\n",
      "Epoch 10, Loss: 0.3380\n",
      "Epoch 11, Loss: 0.3313\n",
      "Epoch 12, Loss: 0.3302\n",
      "Epoch 13, Loss: 0.3168\n",
      "Epoch 14, Loss: 0.3165\n",
      "Epoch 15, Loss: 0.2998\n",
      "Epoch 16, Loss: 0.2962\n",
      "Epoch 17, Loss: 0.2930\n",
      "Epoch 18, Loss: 0.2961\n",
      "Epoch 19, Loss: 0.2923\n",
      "Epoch 20, Loss: 0.2877\n",
      "Test Loss: 0.2443, Test MAE: 0.2405\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==========================\n",
    "# Preprocessing & Data Setup\n",
    "# ==========================\n",
    "\n",
    "# Drop NaN values in vote counts and averages\n",
    "df = df.dropna(subset=['vote_count', 'vote_average'])\n",
    "\n",
    "# Calculate overall mean vote average (C)\n",
    "C = df['vote_average'].mean()\n",
    "\n",
    "# Define m as the 90th percentile of vote_count\n",
    "m = df['vote_count'].quantile(0.90)\n",
    "\n",
    "# We filter out movies that have a vote_count less than m\n",
    "qualified = df[df['vote_count'] >= m].copy()\n",
    "\n",
    "# Function to compute the weighted rating\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m)) * R + (m/(v+m)) * C\n",
    "\n",
    "# We calculate weighted rating and create a new column\n",
    "qualified['weighted_rating'] = qualified.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Sorting movies by weighted rating\n",
    "qualified = qualified.sort_values('weighted_rating', ascending=False)\n",
    "\n",
    "# Generate synthetic user_id and select relevant columns\n",
    "df['user_id'] = np.random.randint(0, 1000, df.shape[0])  # Assign random user IDs\n",
    "\n",
    "ratings = df[['user_id', 'id', 'weighted_rating']].dropna()\n",
    "ratings.rename(columns={'id': 'movie_id'}, inplace=True)\n",
    "\n",
    "# Normalize weighted_rating\n",
    "ratings['weighted_rating'] = (ratings['weighted_rating'] - ratings['weighted_rating'].min()) / \\\n",
    "                             (ratings['weighted_rating'].max() - ratings['weighted_rating'].min())\n",
    "\n",
    "# Clip negative values (if any)\n",
    "ratings['weighted_rating'] = ratings['weighted_rating'].clip(lower=0)\n",
    "\n",
    "# Encode users and movies\n",
    "total_users = ratings['user_id'].nunique()\n",
    "total_movies = ratings['movie_id'].nunique()\n",
    "\n",
    "user2idx = {user: idx for idx, user in enumerate(ratings['user_id'].unique())}\n",
    "movie2idx = {movie: idx for idx, movie in enumerate(ratings['movie_id'].unique())}\n",
    "\n",
    "ratings.loc[:, 'user_id'] = ratings['user_id'].map(user2idx)\n",
    "ratings.loc[:, 'movie_id'] = ratings['movie_id'].map(movie2idx)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==========================\n",
    "# PyTorch Dataset Class\n",
    "# ==========================\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.users = torch.tensor(data['user_id'].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(data['movie_id'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(data['weighted_rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = MovieDataset(train_data)\n",
    "test_dataset = MovieDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# Neural Collaborative Filtering (NCF) Model\n",
    "# ==========================\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embed_size=64):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embed_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embed_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user, movie):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        interaction = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        output = self.fc_layers(interaction)\n",
    "        return output.squeeze()\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ncf_model = NCF(total_users, total_movies).to(device)\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "optimizer = optim.AdamW(ncf_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ==========================\n",
    "# Model Weight Initialization\n",
    "# ==========================\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "ncf_model.apply(weights_init)\n",
    "\n",
    "# ==========================\n",
    "# Training Loop\n",
    "# ==========================\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience, counter = 3, 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for users, movies, ratings in train_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            for param in model.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# ==========================\n",
    "# Evaluation Function\n",
    "# ==========================\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_absolute_error = 0  # Variable to store the sum of absolute errors\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in test_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            predictions = model(users, movies)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate absolute errors for MAE\n",
    "            absolute_error = torch.abs(predictions - ratings)\n",
    "            total_absolute_error += absolute_error.sum().item()\n",
    "    \n",
    "    # Calculate MAE and average loss\n",
    "    mae = total_absolute_error / len(test_loader.dataset)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# Run Training and Evaluation\n",
    "# ==========================\n",
    "\n",
    "train(ncf_model, train_loader, criterion, optimizer, scheduler, epochs=20)\n",
    "evaluate(ncf_model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951672d-b3de-4e3c-be0d-c478ec1b07a3",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) Regressor for Movie Rating Prediction\n",
    "\n",
    "In this code, we implement a **K-Nearest Neighbors (KNN)** model to predict movie ratings using a **weighted rating** system based on movie popularity and user preferences. The process begins by loading and preprocessing the movie dataset, which includes dropping missing values for **`vote_count`** and **`vote_average`**, and calculating a **mean vote average (C)** and the 90th percentile of **vote_count (m)**. Movies with a vote count below **m** are excluded, and a **weighted rating** is calculated for each remaining movie, considering both the movie's rating and its popularity. Next, we simulate user-item interactions by assigning random **user_ids** to each movie, then encode the **user_id** and **movie_id** using **LabelEncoder** for easier manipulation. The **weighted_rating** is normalized using **MinMaxScaler** to ensure the values are between 0 and 1. The data is split into **training** and **test** sets, with **user_id** and **movie_id** as features and the **normalized ratings** as the target. A **KNN Regressor** model is trained on the training set, using **cosine similarity** as the distance metric, and predictions are made on the test set. The model's performance is evaluated using the **Mean Absolute Error (MAE)** metric to measure the prediction accuracy. The output displays the MAE, reflecting the model's ability to predict ratings based on the weighted popularity of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "859e7f17-f528-4766-9d79-aab611badf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.1617\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"MoviesData_Processed.csv\")\n",
    "\n",
    "# Drop NaN values in vote counts and averages\n",
    "df = df.dropna(subset=['vote_count', 'vote_average'])\n",
    "\n",
    "# Calculate overall mean vote average (C)\n",
    "C = df['vote_average'].mean()\n",
    "\n",
    "# Define m as the 90th percentile of vote_count\n",
    "m = df['vote_count'].quantile(0.90)\n",
    "\n",
    "# Filter out movies that have a vote_count less than m\n",
    "qualified = df[df['vote_count'] >= m].copy()\n",
    "\n",
    "# Function to compute the weighted rating\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m)) * R + (m/(v+m)) * C\n",
    "\n",
    "# Calculate weighted rating and create a new column\n",
    "qualified['weighted_rating'] = qualified.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Sorting movies by weighted rating\n",
    "qualified = qualified.sort_values('weighted_rating', ascending=False)\n",
    "\n",
    "# Simulate user-item interactions\n",
    "qualified.loc[:, 'user_id'] = np.random.randint(0, 1000, qualified.shape[0])  # Simulated users\n",
    "ratings = qualified[['user_id', 'id', 'weighted_rating']].dropna()\n",
    "ratings.rename(columns={'id': 'movie_id'}, inplace=True)\n",
    "\n",
    "# Encode users and movies\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "ratings.loc[:, 'user_id'] = user_encoder.fit_transform(ratings['user_id'])\n",
    "ratings.loc[:, 'movie_id'] = movie_encoder.fit_transform(ratings['movie_id'])\n",
    "\n",
    "# Normalize weighted ratings\n",
    "scaler = MinMaxScaler()\n",
    "ratings.loc[:, 'rating'] = scaler.fit_transform(ratings[['weighted_rating']])\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training and test sets\n",
    "X_train = train_data[['user_id', 'movie_id']]\n",
    "y_train = train_data['rating']\n",
    "X_test = test_data[['user_id', 'movie_id']]\n",
    "y_test = test_data['rating']\n",
    "\n",
    "# Train KNN model\n",
    "knn_model = KNeighborsRegressor(n_neighbors=10, metric='cosine')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0c8de-0fd4-4d07-9c73-1352fc1129aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
