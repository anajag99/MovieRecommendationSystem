{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0a9abe-177a-41be-8942-097a455e357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from surprise import Dataset, Reader, SVD, NMF, accuracy\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from sklearn.metrics import mean_squared_error, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# For visualization\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26101be5-e5e8-4d4b-9ecd-835afd3799ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating   timestamp          title       genres\n",
      "0       1       16     4.0  1217897793  Casino (1995)  Crime|Drama\n",
      "1       9       16     4.0   842686699  Casino (1995)  Crime|Drama\n",
      "2      12       16     1.5  1144396284  Casino (1995)  Crime|Drama\n",
      "3      24       16     4.0   963468757  Casino (1995)  Crime|Drama\n",
      "4      29       16     3.0   836820223  Casino (1995)  Crime|Drama\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "title        0\n",
      "genres       0\n",
      "dtype: int64\n",
      "              userId        movieId         rating     timestamp\n",
      "count  105339.000000  105339.000000  105339.000000  1.053390e+05\n",
      "mean      364.924539   13381.312477       3.516850  1.130424e+09\n",
      "std       197.486905   26170.456869       1.044872  1.802660e+08\n",
      "min         1.000000       1.000000       0.500000  8.285650e+08\n",
      "25%       192.000000    1073.000000       3.000000  9.711008e+08\n",
      "50%       383.000000    2497.000000       3.500000  1.115154e+09\n",
      "75%       557.000000    5991.000000       4.000000  1.275496e+09\n",
      "max       668.000000  149532.000000       5.000000  1.452405e+09\n"
     ]
    }
   ],
   "source": [
    "# Load MovieLens Dataset\n",
    "df_ratings = pd.read_csv(\"ratings.csv\")\n",
    "df_movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "# Merge ratings with movies for exploration\n",
    "df = df_ratings.merge(df_movies, on=\"movieId\")\n",
    "\n",
    "# Display dataset info\n",
    "print(df.head())\n",
    "\n",
    "# Check missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic stats\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197ddbed-a550-4518-93d6-b72537fd7e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 668 users, 9504 items\n",
      "Test set size: 21068 ratings\n"
     ]
    }
   ],
   "source": [
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "\n",
    "# Convert ratings to binary (1 = liked, 0 = not liked)\n",
    "df[\"liked\"] = np.where(df[\"rating\"] >= 3.5, 1, 0)\n",
    "\n",
    "# Define Surprise Reader\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# Load data from the DataFrame\n",
    "data = Dataset.load_from_df(df[[\"userId\", \"movieId\", \"rating\"]], reader)\n",
    "\n",
    "# Use Surprise's train_test_split to split the data into training and testing sets\n",
    "trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the size of the trainset and testset\n",
    "print(f\"Training set size: {trainset.n_users} users, {trainset.n_items} items\")\n",
    "print(f\"Test set size: {len(testset)} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9a0aa5-3e3c-4e5d-a8dc-e5ca13fb77d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8726\n",
      "MAE:  0.6720\n"
     ]
    }
   ],
   "source": [
    "# Train Matrix Factorization Models\n",
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "svd = SVD()\n",
    "svd.fit(trainset)\n",
    "predictions_svd = svd.test(testset)\n",
    "\n",
    "# Evaluate SVD\n",
    "rmse_svd = accuracy.rmse(predictions_svd)\n",
    "mae_svd = accuracy.mae(predictions_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27ce6f0-d080-49db-9783-671c3d7fad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9341\n",
      "MAE:  0.7165\n"
     ]
    }
   ],
   "source": [
    "# Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "nmf = NMF()\n",
    "nmf.fit(trainset)\n",
    "predictions_nmf = nmf.test(testset)\n",
    "\n",
    "# Evaluate NMF\n",
    "rmse_nmf = accuracy.rmse(predictions_nmf)\n",
    "mae_nmf = accuracy.mae(predictions_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62796dd5-d808-40f5-8fea-9143aedfc1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 668\n",
      "Test set size: 21068\n",
      "XGBoost RMSE: 0.9447678298359136\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Model\n",
    "\n",
    "# Convert ratings to binary (1 = liked, 0 = not liked)\n",
    "df[\"liked\"] = np.where(df[\"rating\"] >= 3.5, 1, 0)\n",
    "\n",
    "# Define Surprise Reader\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# Load data from the DataFrame\n",
    "data = Dataset.load_from_df(df[[\"userId\", \"movieId\", \"rating\"]], reader)\n",
    "\n",
    "# Use Surprise's train_test_split to split the data into training and testing sets\n",
    "trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the size of the trainset and testset\n",
    "print(f\"Training set size: {len(trainset.all_users())}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "# Prepare data for XGBoost using userId and movieId\n",
    "features = df[[\"userId\", \"movieId\"]]\n",
    "labels = df[\"rating\"]\n",
    "\n",
    "# Train-test split for XGBoost using sklearn\n",
    "X_train, X_test, y_train, y_test = sklearn_train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(f\"XGBoost RMSE: {rmse_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7466d4f7-f19e-4de0-abbc-22bdf55ea915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7129961252212524\n",
      "Epoch 2/5, Loss: 0.7060458064079285\n",
      "Epoch 3/5, Loss: 0.7003841996192932\n",
      "Epoch 4/5, Loss: 0.6954838037490845\n",
      "Epoch 5/5, Loss: 0.6910406351089478\n",
      "Test Accuracy: 0.604138970375061\n"
     ]
    }
   ],
   "source": [
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_size=50):\n",
    "        super(NCFModel, self).__init__()\n",
    "        # Embedding layers for users and movies\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(embedding_size * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, user_input, movie_input):\n",
    "        # Embedding lookups for users and movies\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        movie_embedded = self.movie_embedding(movie_input)\n",
    "        \n",
    "        # Concatenate the embeddings\n",
    "        x = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.output(x))  # Output between 0 and 1 for like/dislike prediction\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Prepare the data\n",
    "df_ratings = pd.read_csv(\"ratings.csv\")\n",
    "df_movies = pd.read_csv(\"movies.csv\")\n",
    "df_ratings['liked'] = np.where(df_ratings['rating'] >= 3.5, 1, 0)\n",
    "\n",
    "# Remap user and movie IDs to consecutive integers starting from 0\n",
    "df_ratings['userId'] = pd.factorize(df_ratings['userId'])[0]\n",
    "df_ratings['movieId'] = pd.factorize(df_ratings['movieId'])[0]\n",
    "\n",
    "# Split data into train and test sets\n",
    "train, test = sklearn_train_test_split(df_ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_users = torch.tensor(train['userId'].values, dtype=torch.long)\n",
    "train_movies = torch.tensor(train['movieId'].values, dtype=torch.long)\n",
    "train_labels = torch.tensor(train['liked'].values, dtype=torch.float32)\n",
    "\n",
    "test_users = torch.tensor(test['userId'].values, dtype=torch.long)\n",
    "test_movies = torch.tensor(test['movieId'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test['liked'].values, dtype=torch.float32)\n",
    "\n",
    "# Initialize the model with updated number of users and movies\n",
    "num_users = df_ratings['userId'].nunique()\n",
    "num_movies = df_ratings['movieId'].nunique()\n",
    "\n",
    "model = NCFModel(num_users=num_users, num_movies=num_movies, embedding_size=50)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(train_users, train_movies).squeeze()\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_users, test_movies).squeeze()\n",
    "    test_predictions = (test_outputs >= 0.5).float()  # Convert probabilities to binary predictions\n",
    "    accuracy = (test_predictions == test_labels).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e416fa-c82a-42a2-90e2-f1ab22324b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based Filtering Metrics:\n",
      "Precision@10: 4.2216\n",
      "Recall@10: 0.3310\n",
      "F1-Score@10: 0.5322\n"
     ]
    }
   ],
   "source": [
    "#content based filtering with evaluation\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to evaluate Precision, Recall, and F1 for content-based filtering\n",
    "def evaluate_content_based_recommendation(user_ratings, top_k=10):\n",
    "    # Initialize Precision, Recall, F1 score lists\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    # For each user, calculate Precision@K, Recall@K, F1-Score\n",
    "    for user_id in user_ratings['userId'].unique():\n",
    "        # Get the ratings of the current user\n",
    "        user_data = user_ratings[user_ratings['userId'] == user_id]\n",
    "        liked_movies = user_data[user_data['rating'] >= 4]['movieId'].values\n",
    "        \n",
    "        # Generate top K recommendations based on content-based model (using nearest neighbors)\n",
    "        top_recommendations = recommend_top_k_movies(user_id, top_k)  # This function needs to return the top K movie ids\n",
    "        \n",
    "        # Check how many of the top K recommended movies are in the list of liked movies\n",
    "        relevant_recommendations = [movie for movie in top_recommendations if movie in liked_movies]\n",
    "        \n",
    "        # Calculate Precision@K, Recall@K\n",
    "        precision = len(relevant_recommendations) / top_k\n",
    "        recall = len(relevant_recommendations) / len(liked_movies) if len(liked_movies) > 0 else 0\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    # Calculate average Precision, Recall, F1 score across all users\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    avg_f1 = np.mean(f1_list)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Example function to recommend top K movies for a user based on content-based filtering\n",
    "def recommend_top_k_movies(user_id, top_k=10):\n",
    "    # Get movies rated by the user\n",
    "    user_data = df_ratings[df_ratings['userId'] == user_id]\n",
    "    rated_movie_ids = user_data['movieId'].values\n",
    "    \n",
    "    # For each movie, compute the cosine similarity with the user's rated movies (Content-Based)\n",
    "    movie_indices = df_movies[df_movies['movieId'].isin(rated_movie_ids)].index.tolist()\n",
    "    distances, indices = nn_model.kneighbors(tfidf_matrix[movie_indices], n_neighbors=top_k)\n",
    "    \n",
    "    # Flatten and get movie indices for top K recommendations\n",
    "    recommended_movies = [df_movies.iloc[i][\"movieId\"] for i in indices.flatten()]\n",
    "    \n",
    "    return recommended_movies\n",
    "\n",
    "# Evaluate the Content-Based Filtering model with Precision@K, Recall@K, F1-Score\n",
    "precision, recall, f1 = evaluate_content_based_recommendation(df_ratings, top_k=10)\n",
    "\n",
    "print(f\"Content-Based Filtering Metrics:\")\n",
    "print(f\"Precision@10: {precision:.4f}\")\n",
    "print(f\"Recall@10: {recall:.4f}\")\n",
    "print(f\"F1-Score@10: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc672c8a-2030-4a6f-b762-ca6d80f62f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Movies:\n",
      "Monsters, Inc. (2001)\n",
      "DuckTales: The Movie - Treasure of the Lost Lamp (1990)\n",
      "Shrek the Third (2007)\n",
      "Antz (1998)\n",
      "Toy Story (1995)\n"
     ]
    }
   ],
   "source": [
    "#Content-Based Filtering (TF-IDF) - without evaluation\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = tfidf.fit_transform(df_movies[\"genres\"])\n",
    "\n",
    "# Nearest Neighbors Model\n",
    "nn_model = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "nn_model.fit(tfidf_matrix)\n",
    "\n",
    "# Function to recommend movies\n",
    "def recommend_movies(movie_title, n=5):\n",
    "    idx = df_movies[df_movies[\"title\"] == movie_title].index[0]\n",
    "    distances, indices = nn_model.kneighbors(tfidf_matrix[idx], n_neighbors=n+1)\n",
    "    \n",
    "    print(\"Recommended Movies:\")\n",
    "    for i in indices.flatten()[1:]:\n",
    "        print(df_movies.iloc[i][\"title\"])\n",
    "\n",
    "# Example Recommendation\n",
    "recommend_movies(\"Toy Story (1995)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7ada0-7ed6-4005-8b55-2d662896f049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
